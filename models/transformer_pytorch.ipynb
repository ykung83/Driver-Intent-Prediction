{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: 'Downloads'\n",
      "C:\\Users\\ykung\\Downloads\n"
     ]
    }
   ],
   "source": [
    "cd Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ykung\\Downloads\\b4c\n"
     ]
    }
   ],
   "source": [
    "cd b4c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ykung\\Downloads\\b4c\\Brains4Cars\n"
     ]
    }
   ],
   "source": [
    "cd Brains4Cars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Straight from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import copy\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "ACTION_TO_ID_MAP= {\n",
    "    'end_action':   0,\n",
    "    'lchange':      1,\n",
    "    'lturn':        2,\n",
    "    'rchange':      3,\n",
    "    'rturn':        4\n",
    "}\n",
    "\n",
    "rng = np.random.default_rng(seed=42)\n",
    "MAX_FRAMES=150\n",
    "\n",
    "\n",
    "class B4CDataset(Dataset):\n",
    "    \"\"\"\n",
    "    end_action - 0\n",
    "    lchange - 1\n",
    "    lturn - 2\n",
    "    rchange - 3\n",
    "    rturn - 4\n",
    "    \"\"\"\n",
    "    def __init__(self, data_cfg, split=\"train\", create_dataset=False):\n",
    "        self.data_cfg   = data_cfg['DATALOADER_CONFIG']\n",
    "        self.actions    = self.data_cfg['ACTIONS']\n",
    "        self.cameras    = self.data_cfg['CAMERAS']\n",
    "        self.data_dir   = self.data_cfg['DATA_DIR']\n",
    "        self.split      = split\n",
    "\n",
    "        videos_dict = self.read_videos_by_action()        \n",
    "\n",
    "        if create_dataset:\n",
    "            # self.create_gt_road_labels(videos_dict)\n",
    "            self.generate_imagesets(videos_dict)\n",
    "        \n",
    "        self.image_sets = {}\n",
    "        splits_process = [self.split] if self.split in [\"train\", \"val\", \"test\"] else [\"train\", \"val\", \"test\"]\n",
    "\n",
    "        for camera in self.cameras:\n",
    "            if camera not in self.image_sets:\n",
    "                self.image_sets[camera] = []\n",
    "\n",
    "            for curr_split in splits_process:\n",
    "                imageset_path = join(self.data_dir, f'ImageSets_{camera}', f'{curr_split}.txt')\n",
    "                self.image_sets[camera].extend([line.strip() for line in open(imageset_path, 'r')])\n",
    "\n",
    "        # Drop nondivisible videos for fold splits from end\n",
    "        if \"fold\" in self.split:\n",
    "            num_folds = int(self.split.split(\"_\")[0])\n",
    "            num_to_drop = len(self) % num_folds\n",
    "            self.image_sets = {k: v[:-num_to_drop] for k, v in self.image_sets.items()}\n",
    "\n",
    "        print(f'Added {len(self)} videos to the dataset.')\n",
    "\n",
    "    def read_videos_by_action(self):\n",
    "        videos_dict = {}\n",
    "        # Combine image sets for all cameras\n",
    "        for camera in self.cameras:\n",
    "            camera_dir = join(self.data_dir, camera+\"_processed\")\n",
    "\n",
    "            for action in self.actions:\n",
    "                action_dir = join(camera_dir, action)\n",
    "\n",
    "                if action not in videos_dict:\n",
    "                    videos_dict[action] = []\n",
    "\n",
    "                # Take the set intersection between all the cameras sequentially\n",
    "                video_action_set = set([f for f in os.listdir(action_dir) if os.path.isdir(join(action_dir, f))])\n",
    "\n",
    "                if len(videos_dict[action])==0:\n",
    "                    videos_dict[action] = video_action_set\n",
    "                else:\n",
    "                    videos_dict[action] = videos_dict[action].intersection(video_action_set)\n",
    "\n",
    "        # Convert videos_dict back to list\n",
    "        for action in self.actions:\n",
    "            videos_dict[action] = list(videos_dict[action])\n",
    "\n",
    "        # Check that files exist\n",
    "        for camera in self.cameras:\n",
    "            camera_dir = join(self.data_dir, camera+\"_processed\")\n",
    "\n",
    "            for action in self.actions:\n",
    "                action_dir = join(camera_dir, action)\n",
    "\n",
    "                for video_dir in videos_dict[action]:\n",
    "                    video_dir = join(action_dir, video_dir)\n",
    "                    assert os.path.exists(video_dir), f\"Video directory {video_dir} does not exist.\"\n",
    "\n",
    "        print(\"Finished reading all valid videos by directory.\")\n",
    "        return videos_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def write_list_to_file(file_path, data_list):\n",
    "        \"\"\"\n",
    "        Create/overwrite a .txt file and write each line of the Python list to a new line in the file.\n",
    "\n",
    "        Parameters:\n",
    "            file_path : str\n",
    "                The path to the .txt file.\n",
    "            data_list : list\n",
    "                The Python list containing data to write to the file.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.writelines(f\"{item}\\n\" for item in data_list)\n",
    "\n",
    "    def check_data_quality(self, video_subdirs, camera):\n",
    "        \"\"\"\n",
    "        Check that all videos have the same number of frames and that the number of frames is less than MAX_FRAMES.\n",
    "\n",
    "        Parameters:\n",
    "            videos_dict : dict\n",
    "                Dictionary containing the list of videos for each action.\n",
    "        \"\"\"\n",
    "\n",
    "        valid_videos_mask = np.zeros(len(video_subdirs), dtype=bool)\n",
    "        for video_idx, video_subdir in enumerate(video_subdirs):\n",
    "            video_path = join(self.data_dir, video_subdir)\n",
    "            data_dict = {}\n",
    "\n",
    "            # Check that all videos have the full frame set\n",
    "            if camera==\"face\":\n",
    "                data_dict['gt_gazepose'] = self.get_face_labels(video_path)\n",
    "                valid_videos_mask[video_idx] = len(data_dict['gt_gazepose'])>=MAX_FRAMES-1 # gazepose has 149\n",
    "            elif camera==\"road\":\n",
    "                data_dict['gt_bbox'], data_dict['gt_lanes'] = self.get_road_labels(video_path)\n",
    "                valid_videos_mask[video_idx] = len(data_dict['gt_bbox'])>=MAX_FRAMES and len(data_dict['gt_lanes'])>=MAX_FRAMES\n",
    "\n",
    "            if valid_videos_mask[video_idx]==0:\n",
    "                print(f'Video {video_subdir} does not have the full frame set, skipping...')\n",
    "\n",
    "        return valid_videos_mask\n",
    "            \n",
    "\n",
    "    def generate_imagesets(self, videos_dict):    \n",
    "        print(\"Generating imagesets...\")\n",
    "        facecam_imageset_dict = {'train': [], 'val': [], 'test': []}\n",
    "        roadcam_imageset_dict = copy.deepcopy(facecam_imageset_dict)\n",
    "        train_pct, val_pct, test_pct = 0.7, 0.15, 0.15\n",
    "        for action, action_videos in videos_dict.items():\n",
    "            road_cam_action_dir = join('road_camera_processed_combined', action)\n",
    "            road_cam_video_labels = np.array([join(road_cam_action_dir, f) for f in action_videos])\n",
    "            road_cam_video_mask = self.check_data_quality(road_cam_video_labels, \"road\")\n",
    "\n",
    "            face_cam_action_dir = join('face_camera_processed', action)\n",
    "            face_cam_video_labels = np.array([join(face_cam_action_dir, f) for f in action_videos])\n",
    "            face_cam_video_mask = self.check_data_quality(face_cam_video_labels, \"face\")\n",
    "\n",
    "            combined_cam_video_mask = np.logical_and(road_cam_video_mask, face_cam_video_mask)\n",
    "            road_cam_video_labels = road_cam_video_labels[combined_cam_video_mask]\n",
    "            face_cam_video_labels = face_cam_video_labels[combined_cam_video_mask]\n",
    "\n",
    "            road_cam_video_labels_sort_idx = np.argsort(road_cam_video_labels)\n",
    "            road_cam_video_labels = road_cam_video_labels[road_cam_video_labels_sort_idx]\n",
    "            face_cam_video_labels = face_cam_video_labels[road_cam_video_labels_sort_idx]\n",
    "\n",
    "            # Ensure file order matches for road and face camera\n",
    "            for i in range(len(road_cam_video_labels)):\n",
    "                assert os.path.basename(road_cam_video_labels[i])==os.path.basename(face_cam_video_labels[i]), 'Video labels do not match'\n",
    "            num_videos = len(road_cam_video_labels)\n",
    "\n",
    "            num_train       = int(num_videos * train_pct)\n",
    "            num_val         = int(num_videos * val_pct)\n",
    "            num_test        = int(num_videos * test_pct)\n",
    "\n",
    "            indices = np.arange(0, num_videos, 1)\n",
    "            rng.shuffle(indices)\n",
    "\n",
    "            videos_indices = {\"train\": [], \"val\": [], \"test\": []}\n",
    "            videos_indices['train'], videos_indices['val'], videos_indices['test'] = indices[:num_train], \\\n",
    "                indices[num_train:num_train+num_val],  indices[num_train+num_val:num_train+num_val+num_test]\n",
    "\n",
    "            for split in facecam_imageset_dict.keys():\n",
    "                roadcam_imageset_dict[split].extend(road_cam_video_labels[videos_indices[split]].tolist())\n",
    "                facecam_imageset_dict[split].extend(face_cam_video_labels[videos_indices[split]].tolist())\n",
    "\n",
    "        # Dump to imageset files for road and face camera\n",
    "        roadcam_imagesets_dir = join(self.data_dir, \"ImageSets_road_camera\")\n",
    "        facecam_imagesets_dir = join(self.data_dir, \"ImageSets_face_camera\")\n",
    "        if not os.path.exists(roadcam_imagesets_dir):\n",
    "            print(f'Video root directory {roadcam_imagesets_dir} does not exist. Creating...')\n",
    "            os.mkdir(roadcam_imagesets_dir)\n",
    "        if not os.path.exists(facecam_imagesets_dir):\n",
    "            print(f'Video root directory {facecam_imagesets_dir} does not exist. Creating...')\n",
    "            os.mkdir(facecam_imagesets_dir)\n",
    "\n",
    "        for split_key in facecam_imageset_dict.keys():\n",
    "            road_cam_split_path = join(roadcam_imagesets_dir, f'{split_key}.txt')\n",
    "            face_cam_split_path = join(facecam_imagesets_dir, f'{split_key}.txt')\n",
    "            print(f'Saving imageset file {split_key} for road {road_cam_split_path} and {face_cam_split_path}')\n",
    "            self.write_list_to_file(road_cam_split_path, roadcam_imageset_dict[split_key])\n",
    "            self.write_list_to_file(face_cam_split_path, facecam_imageset_dict[split_key])\n",
    "\n",
    "    def __len__(self):\n",
    "        num_files = 0\n",
    "        for camera in self.cameras: \n",
    "            assert num_files==0 or num_files==len(self.image_sets[camera]), \"Number files in dataset not correct\"\n",
    "            num_files=len(self.image_sets[camera])\n",
    "        return num_files\n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        # print(data[0][1])\n",
    "        data_batch = [bi[0] for bi in data]\n",
    "        action_batch = [bi[1] for bi in data]\n",
    "        return data_batch, action_batch\n",
    "\n",
    "    def get_face_labels(self, label_dir):  \n",
    "        gazepose_path = join(label_dir, 'gazepose.npy')\n",
    "        assert os.path.exists(gazepose_path), f'Label file {gazepose_path} does not exist'\n",
    "        gt_gazepose = np.load(gazepose_path)\n",
    "\n",
    "        if gt_gazepose.shape[0] < MAX_FRAMES:\n",
    "            # print(f'Gaze pose {gazepose_path} has less than 150 frames, padding with zeros...')\n",
    "            # Pad with zeros\n",
    "            gt_gazepose = np.pad(gt_gazepose, ((0, MAX_FRAMES-gt_gazepose.shape[0]), (0, 0)), mode='constant')\n",
    "        # Assume gt_gazepose is not smaller than MAX_FRAMES\n",
    "        num_frames = min(MAX_FRAMES, gt_gazepose.shape[0])\n",
    "        gt_gazepose = gt_gazepose[:num_frames, :]\n",
    "\n",
    "        return gt_gazepose\n",
    "\n",
    "    def get_road_labels(self, label_dir):\n",
    "        \n",
    "        bbox_file = join(label_dir, 'bbox_labels.pkl')\n",
    "        lane_file = join(label_dir, 'lane_labels.pkl')\n",
    "\n",
    "        assert os.path.exists(bbox_file), f'Label directory {bbox_file} does not exist'\n",
    "        assert os.path.exists(lane_file), f'Label directory {lane_file} does not exist'\n",
    "\n",
    "        # Load bbox detections\n",
    "        with open(bbox_file, 'rb') as f:\n",
    "            gt_bbox = pickle.load(f)\n",
    "        # Load road labels\n",
    "        with open(lane_file, 'rb') as f:\n",
    "            gt_lanes = pickle.load(f)\n",
    "\n",
    "        gt_bbox = gt_bbox[:MAX_FRAMES]\n",
    "        gt_lanes = gt_lanes[:MAX_FRAMES]\n",
    "\n",
    "        return gt_bbox, gt_lanes\n",
    "    \n",
    "    def get_action_label(self, video_dir):\n",
    "        action_label = video_dir.split('/')[-2]\n",
    "        assert action_label in ACTION_TO_ID_MAP.keys(), f'Action {action_label} not in action map'\n",
    "        action_id = ACTION_TO_ID_MAP[action_label]\n",
    "        return action_id\n",
    "\n",
    "    @staticmethod\n",
    "    def combine_img_labels(args):\n",
    "        split_video_path, combined_video_path = args\n",
    "        img_label_files = [f for f in os.listdir(split_video_path) if f.endswith('.pkl')]\n",
    "\n",
    "        MAX_NUM_BBOXES = 5 \n",
    "        full_img_label_np = np.ones((MAX_FRAMES+1, MAX_NUM_BBOXES*5)) * -1\n",
    "        num_img_label_files = len(img_label_files)\n",
    "        for img_label_idx, img_label_file in enumerate(img_label_files):\n",
    "            img_label_path = join(split_video_path, img_label_file)\n",
    "            with open(img_label_path, 'rb') as f:\n",
    "                img_data = pickle.load(f)\n",
    "                IMG_W, IMG_H = 720, 480\n",
    "\n",
    "                #1 Convert to xc, yc, w, h\n",
    "                x1, x2, y1, y2 = img_data['xyxy'][:, 0], img_data['xyxy'][:, 2], img_data['xyxy'][:, 1], img_data['xyxy'][:, 3]\n",
    "                xc = (x1 + x2) / 2\n",
    "                yc = (y1 + y2) / 2\n",
    "                h = (x2 - x1)\n",
    "                w = (y2 - y1)\n",
    "            \n",
    "                #2 Only keep bboxes with h and w < 0.33 (Ignore large bboxes of self)\n",
    "                bbox_size_mask = np.logical_and(h < IMG_W*0.33, w < IMG_H*0.33)\n",
    "\n",
    "                #3 Only keep labels with class_ids = 0, 1, 2, 3, 4 (Ignore 5 Date)\n",
    "                class_ids = img_data['class_id'] \n",
    "                class_ids_mask = np.logical_and(class_ids >= 0, class_ids <= 4)\n",
    "\n",
    "                bbox_mask = np.logical_and(bbox_size_mask, class_ids_mask)\n",
    "                proc_img_label = np.ones((MAX_NUM_BBOXES, 5), dtype=int)*-1 # max of five bbox detections per image\n",
    "\n",
    "                if np.sum(bbox_mask)>0:\n",
    "                    xc = xc[bbox_mask].astype(int)\n",
    "                    yc = yc[bbox_mask].astype(int)\n",
    "                    h = h[bbox_mask].astype(int)\n",
    "                    w = w[bbox_mask].astype(int)\n",
    "                    class_ids = class_ids[bbox_mask]\n",
    "                    gt_objs = np.stack((xc, yc, w, h, class_ids), axis=1)\n",
    "\n",
    "                    #4 Select top 5 largest boxes\n",
    "                    gt_obj_areas = gt_objs[:, 2] * gt_objs[:, 3]\n",
    "                    gt_objs_sort_idx = np.argsort(-gt_obj_areas, kind='stable') # Sort high to low\n",
    "                    num_objs = min(MAX_NUM_BBOXES, len(gt_objs_sort_idx))\n",
    "\n",
    "                    proc_img_label[:num_objs, :] = gt_objs[gt_objs_sort_idx[:num_objs], :]\n",
    "\n",
    "                proc_img_label = proc_img_label.flatten()\n",
    "                proc_img_label = np.expand_dims(proc_img_label, axis=0)\n",
    "                full_img_label_np[img_label_idx] = proc_img_label\n",
    "\n",
    "        assert os.path.exists(combined_video_path), f'Video label directory {combined_video_path} does not exist'\n",
    "        video_path = join(combined_video_path, 'bbox_labels.pkl')\n",
    "        with open(video_path, 'wb') as f:\n",
    "            pickle.dump(full_img_label_np, f)\n",
    "        print(\"Saved combined bbox labels to: \", video_path)\n",
    "\n",
    "        # Load, pad, save lane dets\n",
    "        original_label_dir = split_video_path.replace('road_camera_processed', 'road_camera').replace('labels/', '')\n",
    "        road_path = join(original_label_dir+\".txt\")\n",
    "        gt_lanes = np.loadtxt(road_path, delimiter=',', dtype=int).reshape(1, -1)\n",
    "        gt_lanes_padded = np.ones((MAX_FRAMES, 3)) * -1\n",
    "        num_valid_gt_lanes = min(num_img_label_files, MAX_FRAMES)\n",
    "        gt_lanes_padded[:num_valid_gt_lanes] = np.repeat(gt_lanes, [num_valid_gt_lanes], axis=0)\n",
    "\n",
    "        lanes_path = join(combined_video_path, 'lane_labels.pkl')\n",
    "        with open(lanes_path, 'wb') as f:\n",
    "            pickle.dump(gt_lanes_padded, f)\n",
    "\n",
    "    def create_gt_road_labels(self, videos_dict):\n",
    "        split_video_path_list = []\n",
    "        combined_video_path_list = []\n",
    "        for action, action_videos in videos_dict.items():\n",
    "            for video in action_videos:\n",
    "                video_path = join(self.data_dir, 'road_camera_processed', 'labels', action, video)\n",
    "                if not os.path.exists(video_path):\n",
    "                    print(f'Video path {video_path} does not exist')\n",
    "                    continue\n",
    "                assert os.path.exists(video_path), f'Video path {video_path} does not exist'\n",
    "\n",
    "                video_label_dir = join(self.data_dir, \"road_camera_processed_combined\", action, video)\n",
    "                if not os.path.exists(video_label_dir):\n",
    "                    print(\"Creating directory: \", video_label_dir)\n",
    "                    os.makedirs(video_label_dir)\n",
    "                \n",
    "                split_video_path_list.append(video_path)\n",
    "                combined_video_path_list.append(video_label_dir)\n",
    "            # self.combine_img_labels((split_video_path_list[0], combined_video_path_list[0]))\n",
    "        pool = Pool(processes=16)\n",
    "        for _ in tqdm.tqdm(pool.imap_unordered(self.combine_img_labels, zip(split_video_path_list, \n",
    "            combined_video_path_list)), total=len(split_video_path_list)):\n",
    "            pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_dict = {}\n",
    "        action_id=None\n",
    "\n",
    "        for camera in self.cameras:\n",
    "            video_subdir    = self.image_sets[camera][idx]\n",
    "            video_fulldir   = join(self.data_dir, video_subdir)\n",
    "\n",
    "            if camera == 'face_camera':\n",
    "                data_dict['gt_gazepose'] = self.get_face_labels(video_fulldir)\n",
    "            elif camera == 'road_camera':\n",
    "                # Load all pickle files in the video directory\n",
    "                data_dict['gt_bbox'], data_dict['gt_lanes'] = self.get_road_labels(video_fulldir)\n",
    "\n",
    "            if action_id is None:\n",
    "                action_id = self.get_action_label(video_fulldir)\n",
    "\n",
    "        # Sort dict by key so that it is is consistent\n",
    "        sorted_data_dict = dict(sorted(data_dict.items(), key=lambda item: item[0]))\n",
    "\n",
    "        sorted_gt_np = np.empty((MAX_FRAMES, 0))\n",
    "        # Stack all values from data_dict into a single np array\n",
    "\n",
    "        for _, items in sorted_data_dict.items():\n",
    "            sorted_gt_np = np.hstack((sorted_gt_np, items))\n",
    "\n",
    "        # TOOD: Extract action label from Imageset file\n",
    "        #  150 x [ (5x5) (2) (2) (3) ] # Pad if not enough frames obj_detections, gazepose, lane_detections # 150 x 32\n",
    "        return sorted_gt_np, action_id # processed_input, action_label one hot vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg=None\n",
    "with open(\"C:/Users/ykung/Downloads/lstm_all.yaml\", 'r') as file:\n",
    "    cfg = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make dataset (do once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading all valid videos by directory.\n",
      "Added 585 videos to the dataset.\n"
     ]
    }
   ],
   "source": [
    "dataset = B4CDataset(cfg, split=\"5_fold\", create_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader = DataLoader(dataset, collate_fn=dataset.collate_fn, batch_size=batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading all valid videos by directory.\n",
      "Added 87 videos to the dataset.\n"
     ]
    }
   ],
   "source": [
    "#valdataset = B4CDataset(cfg, split=\"val\", create_dataset=False)\n",
    "#val_loader = DataLoader(valdataset, collate_fn=dataset.collate_fn, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading all valid videos by directory.\n",
      "Added 87 videos to the dataset.\n"
     ]
    }
   ],
   "source": [
    "#testdataset = B4CDataset(cfg, split=\"test\", create_dataset=False)\n",
    "#test_loader = DataLoader(testdataset, collate_fn=dataset.collate_fn, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define your network, loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiheaded attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        #d_model must be divisible by num_heads\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "    \n",
    "#position feedforward\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "    \n",
    "#position encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "#encoder no decoder\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(d_model*150,5)\n",
    "        self.pfc1 = nn.Linear(4,32)\n",
    "        self.pfc2 = nn.Linear(25,16)\n",
    "        self.pfc3 = nn.Linear(3,16)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.pfc1(x[:,:,25:29])\n",
    "        x2 = self.pfc2(x[:,:,0:25])\n",
    "        x3 = self.pfc3(x[:,:,29:32])\n",
    "        #x1 = torch.flatten(x1, start_dim=1)\n",
    "        #x2 = torch.flatten(x2, start_dim=1)\n",
    "        #x3 = torch.flatten(x3, start_dim=1)\n",
    "        x = torch.cat((x1,x2,x3), 2)\n",
    "        attn_output = self.self_attn(x, x, x, mask=None)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldim = 64\n",
    "numheads = 8\n",
    "hiddendim = modeldim*2\n",
    "dropout = 0.1\n",
    "net1 = EncoderLayer(modeldim,numheads,hiddendim,dropout)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net1.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the training loop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# k-split validation\n",
    "totalsize = len(dataset)\n",
    "indices = list(range(0,totalsize))\n",
    "random.shuffle(indices)\n",
    "seg = int(totalsize/10) # number of splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0 #change for each k-validation split\n",
    "\n",
    "trainlefti = indices[0:split*seg]\n",
    "trainrighti = indices[min(split*seg + seg,totalsize):totalsize]\n",
    "traini = trainlefti + trainrighti\n",
    "vali = indices[split*seg:min(totalsize,seg*split+seg)]\n",
    "\n",
    "train_set = torch.utils.data.dataset.Subset(dataset,traini)\n",
    "val_set = torch.utils.data.dataset.Subset(dataset,vali)\n",
    "\n",
    "trainloader = DataLoader(train_set, collate_fn=dataset.collate_fn, batch_size=batch, shuffle=True)\n",
    "valloader = DataLoader(val_set, collate_fn=dataset.collate_fn, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, optimizer, loss_fn, num_epochs):\n",
    "    # Put the network in training mode\n",
    "    net.train()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        for batch_idx, (data, targets) in enumerate(trainloader):\n",
    "            # TODO: zero the parameter gradients + forward pass + loss computation + backward pass + weight update\n",
    "            data = np.array(data)\n",
    "            tdata = torch.tensor(data)\n",
    "            optimizer.zero_grad()\n",
    "            prediction = net(tdata.float())\n",
    "            ttargets = torch.tensor(targets)\n",
    "            loss = loss_fn(prediction, ttargets)           \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(running_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(net, loader):\n",
    "    net.eval()\n",
    "    se = 0 # sum of correct pred\n",
    "    total = 0\n",
    "    f1input = np.zeros(100)\n",
    "    f1target = np.zeros(100)\n",
    "    i1 = 0\n",
    "    for batch_idx, (data, targets) in enumerate(loader):\n",
    "        data = np.array(data)\n",
    "        tdata = torch.tensor(data)\n",
    "        y = net(tdata.float())\n",
    "        f1input[i1] = int(torch.argmax(y))\n",
    "        f1target[i1] = int(targets[0])\n",
    "        i1 += 1\n",
    "        if int(torch.argmax(y)) == targets[0]:\n",
    "            se += 1\n",
    "        total += 1\n",
    "        \n",
    "    f1score = sklearn.metrics.f1_score(f1target[0:i1], f1input[0:i1], average = 'macro')\n",
    "    return float(se/total), f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "15.99802452325821\n",
      "eval: 0.6379310344827587         f1score: 0.42846430592610557\n",
      "1\n",
      "11.680752575397491\n",
      "eval: 0.7241379310344828         f1score: 0.6345560871876661\n",
      "2\n",
      "9.47038346529007\n",
      "eval: 0.6724137931034483         f1score: 0.5089195526695527\n",
      "3\n",
      "8.919783174991608\n",
      "eval: 0.7068965517241379         f1score: 0.5617366605802393\n",
      "4\n",
      "7.104840934276581\n",
      "eval: 0.6896551724137931         f1score: 0.6030213464696224\n",
      "5\n",
      "6.566701889038086\n",
      "eval: 0.7068965517241379         f1score: 0.6053968253968254\n",
      "6\n",
      "6.3038411140441895\n",
      "eval: 0.7241379310344828         f1score: 0.5196428571428571\n",
      "7\n",
      "5.221352398395538\n",
      "eval: 0.6896551724137931         f1score: 0.5444613811692537\n",
      "8\n",
      "4.6505928337574005\n",
      "eval: 0.7068965517241379         f1score: 0.5906015037593985\n",
      "9\n",
      "3.8791748583316803\n",
      "eval: 0.7241379310344828         f1score: 0.6252575884154832\n",
      "10\n",
      "3.3337049782276154\n",
      "eval: 0.7068965517241379         f1score: 0.5890276538804639\n",
      "11\n",
      "3.6203717440366745\n",
      "eval: 0.7068965517241379         f1score: 0.5559802516324256\n",
      "12\n",
      "2.7233291044831276\n",
      "eval: 0.7068965517241379         f1score: 0.5906015037593985\n",
      "13\n",
      "3.7936284989118576\n",
      "eval: 0.7068965517241379         f1score: 0.5923535253227408\n",
      "14\n",
      "2.63713575899601\n",
      "eval: 0.7413793103448276         f1score: 0.6476190476190476\n",
      "15\n",
      "2.9668470919132233\n",
      "eval: 0.7241379310344828         f1score: 0.6239537836634026\n",
      "16\n",
      "2.0685162767767906\n",
      "eval: 0.7413793103448276         f1score: 0.6395433027011974\n",
      "17\n",
      "1.7180224880576134\n",
      "eval: 0.7241379310344828         f1score: 0.6424438029487869\n",
      "18\n",
      "1.7428314536809921\n",
      "eval: 0.7413793103448276         f1score: 0.6437037037037036\n",
      "19\n",
      "1.2586744502186775\n",
      "eval: 0.7068965517241379         f1score: 0.6166291797870744\n",
      "20\n",
      "1.345405675470829\n",
      "eval: 0.7586206896551724         f1score: 0.6554285714285715\n",
      "21\n",
      "1.221812181174755\n",
      "eval: 0.7241379310344828         f1score: 0.6012838609934799\n",
      "22\n",
      "1.0781894102692604\n",
      "eval: 0.7586206896551724         f1score: 0.6906060606060606\n",
      "23\n",
      "0.9919805563986301\n",
      "eval: 0.7068965517241379         f1score: 0.5874274169483565\n",
      "24\n",
      "1.1400487497448921\n",
      "eval: 0.7241379310344828         f1score: 0.5973944294699012\n",
      "25\n",
      "1.0779591985046864\n",
      "eval: 0.7413793103448276         f1score: 0.6709218500797447\n",
      "26\n",
      "0.9550125785171986\n",
      "eval: 0.7241379310344828         f1score: 0.6239537836634026\n",
      "27\n",
      "0.7015160210430622\n",
      "eval: 0.7758620689655172         f1score: 0.7007643928398646\n",
      "28\n",
      "0.7542639300227165\n",
      "eval: 0.7413793103448276         f1score: 0.6353104984683932\n",
      "29\n",
      "0.6788154691457748\n",
      "eval: 0.7068965517241379         f1score: 0.6240669139114653\n",
      "30\n",
      "0.7299566529691219\n",
      "eval: 0.7241379310344828         f1score: 0.6256868586560741\n",
      "31\n",
      "0.5795093812048435\n",
      "eval: 0.7241379310344828         f1score: 0.6226743328910511\n",
      "32\n",
      "0.55573870241642\n",
      "eval: 0.7413793103448276         f1score: 0.6409562445667922\n",
      "33\n",
      "0.5490487851202488\n",
      "eval: 0.7413793103448276         f1score: 0.612209244284716\n",
      "34\n",
      "0.5111967567354441\n",
      "eval: 0.7586206896551724         f1score: 0.6626773226773227\n",
      "35\n",
      "0.49687996320426464\n",
      "eval: 0.7413793103448276         f1score: 0.6093602693602694\n",
      "36\n",
      "0.4428415149450302\n",
      "eval: 0.7241379310344828         f1score: 0.6009562445667923\n",
      "37\n",
      "0.4414920974522829\n",
      "eval: 0.7413793103448276         f1score: 0.6409562445667922\n",
      "38\n",
      "0.41691206581890583\n",
      "eval: 0.7068965517241379         f1score: 0.5830252100840336\n",
      "39\n",
      "0.497787993401289\n",
      "eval: 0.7241379310344828         f1score: 0.6009562445667923\n",
      "40\n",
      "0.4760765116661787\n",
      "eval: 0.7586206896551724         f1score: 0.6866995073891625\n",
      "41\n",
      "0.5689669996500015\n",
      "eval: 0.7413793103448276         f1score: 0.6108753315649867\n",
      "42\n",
      "0.5057273153215647\n",
      "eval: 0.7758620689655172         f1score: 0.6369963369963371\n",
      "43\n",
      "0.3962256647646427\n",
      "eval: 0.7413793103448276         f1score: 0.634065934065934\n",
      "44\n",
      "0.4000217877328396\n",
      "eval: 0.7758620689655172         f1score: 0.6618338298576629\n",
      "45\n",
      "0.33072885125875473\n",
      "eval: 0.7758620689655172         f1score: 0.6580086580086579\n",
      "46\n",
      "0.32914408296346664\n",
      "eval: 0.7758620689655172         f1score: 0.6603910306269652\n",
      "47\n",
      "0.3074263110756874\n",
      "eval: 0.7586206896551724         f1score: 0.6428726337639805\n",
      "48\n",
      "0.27649992518126965\n",
      "eval: 0.7586206896551724         f1score: 0.6428726337639805\n",
      "49\n",
      "0.2731035454198718\n",
      "eval: 0.7758620689655172         f1score: 0.6580086580086579\n",
      "50\n",
      "0.32032132521271706\n",
      "eval: 0.7413793103448276         f1score: 0.634065934065934\n",
      "51\n",
      "0.3061535097658634\n",
      "eval: 0.7413793103448276         f1score: 0.6398095238095238\n",
      "52\n",
      "0.300682058557868\n",
      "eval: 0.7413793103448276         f1score: 0.6316883116883116\n",
      "53\n",
      "0.2789482483640313\n",
      "eval: 0.7586206896551724         f1score: 0.6211320754716981\n",
      "54\n",
      "0.22291874047368765\n",
      "eval: 0.7586206896551724         f1score: 0.6428726337639805\n",
      "55\n",
      "0.22220145631581545\n",
      "eval: 0.7413793103448276         f1score: 0.6102564102564103\n",
      "56\n",
      "0.19814351480454206\n",
      "eval: 0.7413793103448276         f1score: 0.6025217565709982\n",
      "57\n",
      "0.20165686402469873\n",
      "eval: 0.7413793103448276         f1score: 0.604093567251462\n",
      "58\n",
      "0.20099057350307703\n",
      "eval: 0.7413793103448276         f1score: 0.6025217565709982\n",
      "59\n",
      "0.1925692344084382\n",
      "eval: 0.7758620689655172         f1score: 0.6347089947089947\n",
      "60\n",
      "0.18658684100955725\n",
      "eval: 0.7413793103448276         f1score: 0.6025217565709982\n",
      "61\n",
      "0.20149165578186512\n",
      "eval: 0.7241379310344828         f1score: 0.5919969685486927\n",
      "62\n",
      "0.1817718744277954\n",
      "eval: 0.7586206896551724         f1score: 0.6428726337639805\n",
      "63\n",
      "0.16642968356609344\n",
      "eval: 0.7586206896551724         f1score: 0.6428726337639805\n",
      "64\n",
      "0.17758126463741064\n",
      "eval: 0.7241379310344828         f1score: 0.5919969685486927\n",
      "65\n",
      "0.17239006888121367\n",
      "eval: 0.7413793103448276         f1score: 0.6025217565709982\n",
      "66\n",
      "0.15632528811693192\n",
      "eval: 0.7241379310344828         f1score: 0.5919969685486927\n",
      "67\n",
      "0.17438941076397896\n",
      "eval: 0.7241379310344828         f1score: 0.5935849056603774\n",
      "68\n",
      "0.15389030240476131\n",
      "eval: 0.7413793103448276         f1score: 0.6087993818216942\n",
      "69\n",
      "0.15190409682691097\n",
      "eval: 0.7241379310344828         f1score: 0.5935849056603774\n",
      "70\n",
      "0.14675130788236856\n",
      "eval: 0.7413793103448276         f1score: 0.6025217565709982\n",
      "71\n",
      "0.13356806384399533\n",
      "eval: 0.7241379310344828         f1score: 0.5919969685486927\n",
      "72\n",
      "0.13718567695468664\n",
      "eval: 0.7241379310344828         f1score: 0.5935849056603774\n",
      "73\n",
      "0.125752420630306\n",
      "eval: 0.7586206896551724         f1score: 0.6196892762410003\n",
      "74\n",
      "0.13393946550786495\n",
      "eval: 0.7413793103448276         f1score: 0.6025217565709982\n",
      "75\n",
      "0.13325854251161218\n",
      "eval: 0.7758620689655172         f1score: 0.6369963369963371\n",
      "76\n",
      "0.12270916532725096\n",
      "eval: 0.7413793103448276         f1score: 0.6025217565709982\n",
      "77\n",
      "0.11661502113565803\n",
      "eval: 0.7586206896551724         f1score: 0.6196892762410003\n",
      "78\n",
      "0.12306320015341043\n",
      "eval: 0.7413793103448276         f1score: 0.6025217565709982\n",
      "79\n",
      "0.12530630361288786\n",
      "eval: 0.7586206896551724         f1score: 0.6196892762410003\n",
      "80\n",
      "0.12453795224428177\n",
      "eval: 0.7586206896551724         f1score: 0.6196892762410003\n",
      "81\n",
      "0.11277008522301912\n",
      "eval: 0.7413793103448276         f1score: 0.6025217565709982\n",
      "82\n",
      "0.11826361017301679\n",
      "eval: 0.7586206896551724         f1score: 0.6196892762410003\n",
      "83\n",
      "0.10767848929390311\n",
      "eval: 0.7413793103448276         f1score: 0.6025217565709982\n",
      "84\n",
      "0.09957716101780534\n",
      "eval: 0.7413793103448276         f1score: 0.6025217565709982\n",
      "85\n",
      "0.10173247428610921\n",
      "eval: 0.7586206896551724         f1score: 0.6196892762410003\n",
      "86\n",
      "0.09466897789388895\n",
      "eval: 0.7413793103448276         f1score: 0.6087993818216942\n",
      "87\n",
      "0.09987617982551455\n",
      "eval: 0.7413793103448276         f1score: 0.6087993818216942\n",
      "88\n",
      "0.09637272590771317\n",
      "eval: 0.7413793103448276         f1score: 0.6025217565709982\n",
      "89\n",
      "0.09629542706534266\n",
      "eval: 0.7586206896551724         f1score: 0.6196892762410003\n",
      "90\n",
      "0.09689651150256395\n",
      "eval: 0.7586206896551724         f1score: 0.6196892762410003\n",
      "91\n",
      "0.09728495916351676\n",
      "eval: 0.7758620689655172         f1score: 0.6332769464324441\n",
      "92\n",
      "0.10036598471924663\n",
      "eval: 0.7586206896551724         f1score: 0.6160986758082947\n",
      "93\n",
      "0.09273363603278995\n",
      "eval: 0.7586206896551724         f1score: 0.6211320754716981\n",
      "94\n",
      "0.09399038646370173\n",
      "eval: 0.7586206896551724         f1score: 0.6196892762410003\n",
      "95\n",
      "0.09592216787859797\n",
      "eval: 0.7586206896551724         f1score: 0.6196892762410003\n",
      "96\n",
      "0.09205052140168846\n",
      "eval: 0.7586206896551724         f1score: 0.6196892762410003\n",
      "97\n",
      "0.09277972439303994\n",
      "eval: 0.7586206896551724         f1score: 0.6196892762410003\n",
      "98\n",
      "0.08171299425885081\n",
      "eval: 0.7586206896551724         f1score: 0.6176577808156756\n",
      "99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07933694263920188\n",
      "eval: 0.7586206896551724         f1score: 0.6196892762410003\n"
     ]
    }
   ],
   "source": [
    "for i in range(100): # for train/val stationary time before manuever\n",
    "    print(i)\n",
    "    train(net1,trainloader,optimizer,loss_fn,1)\n",
    "    evalval, f1val = eval(net1,valloader)\n",
    "    print('eval: '+str(evalval) + '         f1score: ' + str(f1val))\n",
    "    if evalval > 0.92:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net1.state_dict(), \"C:/Users/ykung/Downloads/b4c/LSTMFull91\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the values recorded from the best models from each split\n",
    "recacc = [0.89655,0.89655,0.8793,0.862,0.8448,0.8793,0.8448,0.8621,0.9138,0.8448]\n",
    "recf1 = [0.9106,0.84262,0.870255,0.8374,0.8449,0.88,0.874,0.8202,0.8905,0.79139]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8724000000000001\n",
      "0.02339904912597944\n",
      "0.8561865000000001\n",
      "0.03373211185576733\n"
     ]
    }
   ],
   "source": [
    "print(np.average(recacc))\n",
    "print(np.std(recacc))\n",
    "print(np.average(recf1))\n",
    "print(np.std(recf1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Varying time to manuever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(net, trainloader, optimizer, loss_fn, num_epochs):\n",
    "    # Put the network in training mode\n",
    "    net.train()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        for batch_idx, (data, targets) in enumerate(trainloader):\n",
    "            # TODO: zero the parameter gradients + forward pass + loss computation + backward pass + weight update\n",
    "            data = np.array(data)\n",
    "            tdata = torch.tensor(data)\n",
    "            ttargets = torch.tensor(targets)\n",
    "            zten = torch.zeros((len(data),30,32))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            prediction = net(tdata)\n",
    "            loss = loss_fn(prediction, ttargets)           \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            prediction = net(torch.cat((zten,tdata[:,0:120,:]), 1))\n",
    "            loss = loss_fn(prediction, ttargets)           \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "                             \n",
    "            optimizer.zero_grad()\n",
    "            prediction = net(torch.cat((zten,zten,tdata[:,0:90,:]), 1))\n",
    "            loss = loss_fn(prediction, ttargets)           \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            prediction = net(torch.cat((zten,zten,zten,tdata[:,0:60,:]), 1))\n",
    "            loss = loss_fn(prediction, ttargets)           \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "                             \n",
    "            optimizer.zero_grad()\n",
    "            prediction = net(torch.cat((zten,zten,zten,zten,tdata[:,0:30,:]), 1))\n",
    "            loss = loss_fn(prediction, ttargets)           \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "                             \n",
    "        print(running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "84.34658307743275\n",
      "eval: 0.3448275862068966         f1score: 0.10256410256410257\n",
      "1\n",
      "79.30189798016265\n",
      "eval: 0.3448275862068966         f1score: 0.10256410256410257\n",
      "2\n",
      "77.39387576021282\n",
      "eval: 0.39655172413793105         f1score: 0.21077625570776254\n",
      "3\n",
      "74.902875383899\n",
      "eval: 0.43103448275862066         f1score: 0.25396825396825395\n",
      "4\n",
      "73.72999251237188\n",
      "eval: 0.4482758620689655         f1score: 0.2849348435026184\n",
      "5\n",
      "72.08498041827535\n",
      "eval: 0.5344827586206896         f1score: 0.4403404791929382\n",
      "6\n",
      "71.00632982535292\n",
      "eval: 0.5517241379310345         f1score: 0.4097716718266254\n",
      "7\n",
      "70.45337460459773\n",
      "eval: 0.5         f1score: 0.41238702817650186\n",
      "8\n",
      "69.05921841629018\n",
      "eval: 0.603448275862069         f1score: 0.4826839826839827\n",
      "9\n",
      "68.15351578436348\n",
      "eval: 0.6379310344827587         f1score: 0.5632851359167148\n",
      "10\n",
      "67.1173762313591\n",
      "eval: 0.6896551724137931         f1score: 0.6489137336093858\n",
      "11\n",
      "66.36479803751492\n",
      "eval: 0.7068965517241379         f1score: 0.6834023119737405\n",
      "12\n",
      "65.91238094103313\n",
      "eval: 0.6551724137931034         f1score: 0.628061224489796\n",
      "13\n",
      "64.88900881167227\n",
      "eval: 0.6724137931034483         f1score: 0.6326105862947967\n",
      "14\n",
      "64.4273060236697\n",
      "eval: 0.6896551724137931         f1score: 0.6606022962544702\n",
      "15\n",
      "65.75014602956219\n",
      "eval: 0.7241379310344828         f1score: 0.6847003675195894\n",
      "16\n",
      "65.14401444394545\n",
      "eval: 0.7068965517241379         f1score: 0.6851229380641146\n",
      "17\n",
      "63.26116579785541\n",
      "eval: 0.7586206896551724         f1score: 0.7375409796721626\n",
      "18\n",
      "62.84681876573923\n",
      "eval: 0.7068965517241379         f1score: 0.6800746232938042\n",
      "19\n",
      "62.80192767719261\n",
      "eval: 0.6551724137931034         f1score: 0.6326617409324176\n",
      "20\n",
      "62.64518314470146\n",
      "eval: 0.7068965517241379         f1score: 0.6574987974987976\n",
      "21\n",
      "62.54364008094189\n",
      "eval: 0.7586206896551724         f1score: 0.7177293530268358\n",
      "22\n",
      "61.16070106855172\n",
      "eval: 0.7068965517241379         f1score: 0.6641929499072357\n",
      "23\n",
      "61.565246819573346\n",
      "eval: 0.6724137931034483         f1score: 0.6702631578947369\n",
      "24\n",
      "61.22614789071163\n",
      "eval: 0.6206896551724138         f1score: 0.6406088622355488\n",
      "25\n",
      "62.6627065435614\n",
      "eval: 0.7241379310344828         f1score: 0.7059262674834771\n",
      "26\n",
      "60.88846937139781\n",
      "eval: 0.7586206896551724         f1score: 0.7455987865562334\n",
      "27\n",
      "60.68542683011202\n",
      "eval: 0.7758620689655172         f1score: 0.733852813852814\n",
      "28\n",
      "60.62630732311219\n",
      "eval: 0.7241379310344828         f1score: 0.6931685273790537\n",
      "29\n",
      "60.939068049737244\n",
      "eval: 0.7413793103448276         f1score: 0.7253401451049261\n",
      "30\n",
      "60.63088911101192\n",
      "eval: 0.6896551724137931         f1score: 0.6615872662629088\n",
      "31\n",
      "60.41624020890266\n",
      "eval: 0.7586206896551724         f1score: 0.7521770762896833\n",
      "32\n",
      "59.95846186233558\n",
      "eval: 0.7413793103448276         f1score: 0.7119672860798931\n",
      "33\n",
      "59.31979507175866\n",
      "eval: 0.7413793103448276         f1score: 0.7271666481213604\n",
      "34\n",
      "59.23145506060085\n",
      "eval: 0.7758620689655172         f1score: 0.7475174101261057\n",
      "35\n",
      "59.00796258251132\n",
      "eval: 0.7241379310344828         f1score: 0.6931685273790537\n",
      "36\n",
      "59.22392273708975\n",
      "eval: 0.6551724137931034         f1score: 0.6244703942182934\n",
      "37\n",
      "59.69771844984951\n",
      "eval: 0.7413793103448276         f1score: 0.7100000000000001\n",
      "38\n",
      "59.440307470344706\n",
      "eval: 0.7241379310344828         f1score: 0.6816423965050967\n",
      "39\n",
      "60.35373413010938\n",
      "eval: 0.7068965517241379         f1score: 0.6744973191729617\n",
      "40\n",
      "60.42536405992834\n",
      "eval: 0.7413793103448276         f1score: 0.7033445241646712\n",
      "41\n",
      "58.73543567084721\n",
      "eval: 0.7413793103448276         f1score: 0.6943482190324295\n",
      "42\n",
      "58.72835173379139\n",
      "eval: 0.7586206896551724         f1score: 0.7248720285562391\n",
      "43\n",
      "58.34864671703064\n",
      "eval: 0.7068965517241379         f1score: 0.7054911436033187\n",
      "44\n",
      "58.51021489776904\n",
      "eval: 0.7758620689655172         f1score: 0.7651243314468199\n",
      "45\n",
      "58.572985814662744\n",
      "eval: 0.6896551724137931         f1score: 0.6600956937799042\n",
      "46\n",
      "57.8294188169644\n",
      "eval: 0.7068965517241379         f1score: 0.6928654970760234\n",
      "47\n",
      "58.37111347039633\n",
      "eval: 0.7586206896551724         f1score: 0.7322875816993463\n",
      "48\n",
      "57.84202942874293\n",
      "eval: 0.7068965517241379         f1score: 0.7039115133232781\n",
      "49\n",
      "57.93770990179393\n",
      "eval: 0.7931034482758621         f1score: 0.7717543859649122\n"
     ]
    }
   ],
   "source": [
    "for i in range(50): # train/val for varying time to manuever\n",
    "    print(i)\n",
    "    train2(net1,trainloader,optimizer,loss_fn,1)\n",
    "    evalval, f1val = eval(net1,valloader)\n",
    "    print('eval: '+str(evalval) + '         f1score: ' + str(f1val))\n",
    "    if evalval > 0.80:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval2(net, loader):\n",
    "    net.eval()\n",
    "    t2m = np.zeros(1,100)\n",
    "    totalcorrect = np.zeros((1,5,100))\n",
    "    i1 = 0\n",
    "    for batch_idx, (data, targets) in enumerate(loader):\n",
    "        data = np.array(data)\n",
    "        tdata = torch.tensor(data)\n",
    "        zten = torch.zeros((len(data),30,32))\n",
    "        \n",
    "        y = net(torch.cat((zten,zten,zten,zten,tdata[:,0:30,:]), 1))\n",
    "        if int(torch.argmax(y)) == targets[0]:\n",
    "            t2m[i1] = 5\n",
    "            totalcorrect[0,i1] = 1\n",
    "            \n",
    "        y = net(torch.cat((zten,zten,zten,tdata[:,0:60,:]), 1))\n",
    "        if int(torch.argmax(y)) == targets[0]:\n",
    "            if t2m[i1] == 0:\n",
    "                t2m[i1] = 4\n",
    "            totalcorrect[1,i1] = 1\n",
    "            \n",
    "        y = net(torch.cat((zten,zten,tdata[:,0:90,:]), 1))\n",
    "        if int(torch.argmax(y)) == targets[0]:\n",
    "            if t2m[i1] == 0:\n",
    "                t2m[i1] = 3\n",
    "            totalcorrect[2,i1] = 1\n",
    "            \n",
    "        y = net(torch.cat((zten,tdata[:,0:120,:]), 1))\n",
    "        if int(torch.argmax(y)) == targets[0]:\n",
    "            if t2m[i1] == 0:\n",
    "                t2m[i1] = 2\n",
    "            totalcorrect[3,i1] = 1\n",
    "            \n",
    "        y = net(tdata)\n",
    "        if int(torch.argmax(y)) == targets[0]:\n",
    "            if t2m[i1] == 0:\n",
    "                t2m[i1] = 1\n",
    "            totalcorrect[4,i1] = 1\n",
    "            \n",
    "        i1+=1\n",
    "        \n",
    "    return t2m[0:i1], totalcorrect[:,0:i1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_maneuver, correct_expansion = eval2(net1, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total maneuvers: 58\n",
      "Percentage correct 5 s in advance: 0.5689655172413793\n",
      "Percentage correct 4 s in advance: 0.24\n",
      "Percentage correct 3 s in advance: 0.21052631578947367\n",
      "Percentage correct 2 s in advance: 0.4666666666666667\n",
      "Percentage correct 1 s in advance: 0.25\n",
      "Avg time for correct prediction: 4.34\n"
     ]
    }
   ],
   "source": [
    "lt2m = len(time_to_maneuver)\n",
    "print('Total maneuvers: ' + str(lt2m))\n",
    "instancescorrect = 0\n",
    "for i in time_to_maneuver:\n",
    "    if i == 5:\n",
    "        instancescorrect += 1\n",
    "\n",
    "per5 = instancescorrect/lt2m\n",
    "avgtime = 5*instancescorrect\n",
    "print('Percentage correct 5 s in advance: ' + str(per5))\n",
    "\n",
    "numleft = lt2m - instancescorrect\n",
    "instancescorrect = 0\n",
    "for i in time_to_maneuver:\n",
    "    if i == 4:\n",
    "        instancescorrect += 1\n",
    "\n",
    "per4 = instancescorrect/numleft\n",
    "avgtime = avgtime + 4*instancescorrect\n",
    "print('Percentage correct 4 s in advance: ' + str(per4))\n",
    "\n",
    "numleft = numleft - instancescorrect\n",
    "instancescorrect = 0\n",
    "for i in time_to_maneuver:\n",
    "    if i == 3:\n",
    "        instancescorrect += 1\n",
    "\n",
    "per3 = instancescorrect/numleft\n",
    "avgtime = avgtime + 3*instancescorrect\n",
    "print('Percentage correct 3 s in advance: ' + str(per3))\n",
    "\n",
    "numleft = numleft - instancescorrect\n",
    "instancescorrect = 0\n",
    "for i in time_to_maneuver:\n",
    "    if i == 2:\n",
    "        instancescorrect += 1\n",
    "\n",
    "per2 = instancescorrect/numleft\n",
    "avgtime = avgtime + 2*instancescorrect\n",
    "print('Percentage correct 2 s in advance: ' + str(per2))\n",
    "\n",
    "numleft = numleft - instancescorrect\n",
    "instancescorrect = 0\n",
    "for i in time_to_maneuver:\n",
    "    if i == 1:\n",
    "        instancescorrect += 1\n",
    "\n",
    "per1 = instancescorrect/numleft\n",
    "avgtime = avgtime + 1*instancescorrect\n",
    "print('Percentage correct 1 s in advance: ' + str(per1))\n",
    "\n",
    "avgtime = avgtime/(lt2m-numleft)\n",
    "print('Avg time for correct prediction: ' + str(avgtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillerarray = np.zeros((1,len(time_to_maneuver)))\n",
    "fillerarray[0,:] = time_to_maneuver\n",
    "tim2mset = np.concatenate((tim2mset,fillerarray),axis=0)\n",
    "fillerarray = np.zeros((1,5,len(time_to_maneuver)))\n",
    "fillerarray[0,:,:] = correct_expansion\n",
    "expandedset = np.concatenate((expandedset,fillerarray),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 58)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tim2mset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5, 58)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(expandedset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"C:/Users/ykung/Downloads/b4c/timevaryingtim2mset\", tim2mset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"C:/Users/ykung/Downloads/b4c/expandedset\", expandedset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5s pecentage: 0.6310344827586207\n",
      "4s pecentage: 0.2616822429906542\n",
      "3s pecentage: 0.3037974683544304\n",
      "2s pecentage: 0.34545454545454546\n",
      "1s pecentage: 0.25\n",
      "total avg pred time: 4.357414448669202\n"
     ]
    }
   ],
   "source": [
    "num5s = 0\n",
    "num4s = 0\n",
    "num3s = 0\n",
    "num2s = 0\n",
    "num1s = 0\n",
    "total = 580\n",
    "tolcorr = 0\n",
    "for i in range(10):\n",
    "    for j in range(58):\n",
    "        if tim2mset[i,j] == 5:\n",
    "            num5s += 1\n",
    "            tolcorr += 1\n",
    "        elif tim2mset[i,j] == 4:\n",
    "            num4s += 1\n",
    "            tolcorr += 1\n",
    "        elif tim2mset[i,j] == 3:\n",
    "            num3s += 1\n",
    "            tolcorr += 1\n",
    "        elif tim2mset[i,j] == 2:\n",
    "            num2s += 1\n",
    "            tolcorr += 1\n",
    "        elif tim2mset[i,j] == 1:\n",
    "            num1s += 1\n",
    "            tolcorr += 1\n",
    "print('5s pecentage: ' + str(num5s/total))\n",
    "print('4s pecentage: ' + str(num4s/(total-num5s)))\n",
    "print('3s pecentage: ' + str(num3s/(total-num5s-num4s)))\n",
    "print('2s pecentage: ' + str(num2s/(total-num5s-num4s-num3s)))\n",
    "print('1s pecentage: ' + str(num1s/(total-num5s-num4s-num3s-num2s)))\n",
    "print('total avg pred time: ' + str((num5s*5+num4s*4+num3s*3+num2s*2+num1s)/tolcorr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "expandedset = np.load(\"C:/Users/ykung/Downloads/b4c/expandedset.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5, 58)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(expandedset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5s pecentage: 0.6310344827586207\n",
      "4s pecentage: 0.6793103448275862\n",
      "3s pecentage: 0.746551724137931\n",
      "2s pecentage: 0.8086206896551724\n",
      "1s pecentage: 0.8241379310344827\n"
     ]
    }
   ],
   "source": [
    "num5s = 0\n",
    "num4s = 0\n",
    "num3s = 0\n",
    "num2s = 0\n",
    "num1s = 0\n",
    "total = 580\n",
    "for i in range(10):\n",
    "    for j in range(58):\n",
    "        if expandedset[i,0,j] == 1:\n",
    "            num5s += 1\n",
    "            \n",
    "        if expandedset[i,1,j] == 1:\n",
    "            num4s += 1\n",
    "            \n",
    "        if expandedset[i,2,j] == 1:\n",
    "            num3s += 1\n",
    "            \n",
    "        if expandedset[i,3,j] == 1:\n",
    "            num2s += 1\n",
    "            \n",
    "        if expandedset[i,4,j] == 1:\n",
    "            num1s += 1\n",
    "            \n",
    "print('5s pecentage: ' + str(num5s/total))\n",
    "print('4s pecentage: ' + str(num4s/total))\n",
    "print('3s pecentage: ' + str(num3s/total))\n",
    "print('2s pecentage: ' + str(num2s/total))\n",
    "print('1s pecentage: ' + str(num1s/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3e1d7de5394ae006cb1b050b790a43d094ee6fb03cecb31a31118734e6247e4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
